{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96a4911-2cfc-4234-9098-453018ec4436",
   "metadata": {},
   "source": [
    "# Inference from any pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd975a-3960-43ce-8ede-2ac20fb3af2c",
   "metadata": {},
   "source": [
    "With this notebook you can run the inference of ContourCraft using an arbitrary (for example, non-SMPL(-X)) mesh sequence.\n",
    "\n",
    "To run it, you will need a mesh sequence as a `.pkl` file containing a dictionary of the following items (see `DEFAULTS.data_root/examples/fromanypose/mesh_sequence.pkl` as an example):\n",
    "* `verts`: np.array [N, V, 3]\n",
    "* `faces`: np.array [F, 3]\n",
    "\n",
    "You will also need a garment mesh (as `.obj` file) which is aligned with the first frame of your body sequence.\n",
    "\n",
    "In this notebook we first show how to convert a garment mesh in `.obj` format into a garment dict used in ContourCraft. \n",
    "\n",
    "Then, we show how to use the configuration file `aux/from_any_pose.yaml` to run inference over an arbitrary mesh sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91eba6-68a4-401b-bbfe-33292ca5ba4b",
   "metadata": {},
   "source": [
    "## Create template file from the .obj file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b41a41-90b3-48b5-8447-858797c9dd53",
   "metadata": {},
   "source": [
    "Use `utils.mesh_creation::obj2template()` function to convert an `.obj` file into a template dictionary and then save it with `pickle_dump`\n",
    "\n",
    "`DEFAULTS.data_root/examples/fromanypose/skirt.obj` is provided as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41fed68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/1lh6p1hx46n_8jppf9dnjmth0000gn/T/tmp6crc22i0\n"
     ]
    }
   ],
   "source": [
    "from utils.mesh_creation import obj2template\n",
    "from utils.io import pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.show import write_video \n",
    "from aitviewer.headless import HeadlessRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8bbd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding coarse edges... (may take a while)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "obj_path = Path(DEFAULTS.data_root) / 'examples' / 'fromanypose' / 'skirt.obj'\n",
    "out_template_path = Path(DEFAULTS.data_root) / 'examples' / 'fromanypose' / 'skirt.pkl'\n",
    "\n",
    "template_dict = obj2template(obj_path, verbose=True)\n",
    "pickle_dump(template_dict, out_template_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8536a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mesh_creation import add_pinned_verts\n",
    "\n",
    "\n",
    "pinned_indices = \\\n",
    "[2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215,\n",
    " 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235,\n",
    " 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2327, 2328, 2351, 2492, 2497, 2669, 2670, 2671, 2674, 2837, 3139,\n",
    " 3200, 3204, 3359, 3362, 3495, 3512, 3634, 3638, 3805, 3965, 3967, 4133, 4137, 4140, 4335, 4340, 4506, 4509, 4669, 4674,\n",
    " 4749, 4812, 4849, 4853, 5138, 5309, 5342, 5469, 5474, 5503, 5646, 5650, 5654, 5855, 5857, 6028, 6091, 6204, 6209, 6280,\n",
    " 6374, 6377, 6378, 6473, 6649, 6654, 6814, 6817, 6986, 6989, 6990, 6992, 7172, 7178, 7336, 7500, 7505, 7661, 7665, 7666]\n",
    "\n",
    "add_pinned_verts(out_template_path, pinned_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc63433-3d5a-4583-9636-1562ea1f62f2",
   "metadata": {},
   "source": [
    "## Inference with a mesh sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabf78b-9afb-409a-8cda-3433e9b93308",
   "metadata": {},
   "source": [
    "Here we use an sequence of arbitrary meshes.  The garment template needs to be aligned with the first frame of the sequence.\n",
    "\n",
    "The mesh sequence has to be a `.pkl` file containing a dictionary of the following items (see `DEFAULTS.data_root/examples/fromanypose/mesh_sequence.pkl` as an example):\n",
    "* `verts`: np.array [N, V, 3]\n",
    "* `faces`: np.array [F, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facedbd7-2d8b-4131-bcc9-a9c688c5b5ed",
   "metadata": {},
   "source": [
    "### Load a runner and a dataloader from `from_any_pose` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a143d7ec-7e0e-4765-86b4-463eaed6baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.arguments import load_params, create_modules\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device\n",
    "from utils.io import pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from utils.arguments import create_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07db298-afb1-4b72-af26-7842a6947626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warp 1.10.0 initialized:\n",
      "   CUDA not enabled in this build\n",
      "   Devices:\n",
      "     \"cpu\"      : \"arm\"\n",
      "   Kernel cache:\n",
      "     /Users/jan.rojc/Library/Caches/warp/1.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use HOOD\n",
    "# modules, config = load_params('aux/from_any_pose_hood')\n",
    "# checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'hood_final.pth'\n",
    "\n",
    "# use ContourCraft\n",
    "modules, config = load_params('aux/from_any_pose')\n",
    "checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'contourcraft.pth'\n",
    "\n",
    "\n",
    "runner_module, runner, aux_modules = create_runner(modules, config)\n",
    "\n",
    "# NOCUDA fix\n",
    "# state_dict =  torch.load(checkpoint_path)\n",
    "state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "runner.load_state_dict(state_dict['training_module'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56133fff",
   "metadata": {},
   "source": [
    "### Create a dataloader\n",
    "\n",
    "Same as in the previous part, set several variables to specify which garment and which pose sequence you want to use:\n",
    "\n",
    "\n",
    "- `pose_sequence_path`: path to the smpl pose sequence relative to `DEFAULTS.data_root`. For this example we use `examples/fromanypose/mesh_sequence.pkl`\n",
    "- `garment_template_path`: path to the .pkl file with the garment template. For this example we use `examples/fromanypose/skirt.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae7ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import make_fromanypose_dataloader\n",
    "\n",
    "pose_sequence_path = Path('examples') / 'fromanypose' / 'mesh_sequence.pkl'\n",
    "# garment_template_path = Path('examples') / 'fromanypose' / 'skirt.pkl'\n",
    "# pose_sequence_path = Path('examples') / 'fromanypose' / 'mesh_sequence.pkl'\n",
    "garment_template_path = Path('aux_data') / 'garment_dicts' / 'smpl' / 'aaron_009::bottom.pkl'\n",
    "\n",
    "\n",
    "dataloader = make_fromanypose_dataloader(pose_sequence_type='mesh', \n",
    "                       pose_sequence_path=pose_sequence_path, \n",
    "                       garment_template_path=garment_template_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a43ad4-5bd9-4ae9-939b-f804dea17dd0",
   "metadata": {},
   "source": [
    "### load sample, infer and save trajectories\n",
    "\n",
    "To visualise the saved trajectories, see `Inference.ipynb::write_video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81b569-69d4-4989-9035-6127d39e26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataloader))\n",
    "\n",
    "# NOCUDA fix\n",
    "# sample = move2device(sample, 'cuda:0')\n",
    "# runner.to('cuda:0')\n",
    "sample = move2device(sample, 'cpu')\n",
    "runner.to('cpu')\n",
    "\n",
    "trajectories_dict = runner.valid_rollout(sample, n_steps=100)\n",
    "\n",
    "# Save the sequence to disk\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output_m.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful!: creating more that one renderer in a single session causes an error\n",
    "renderer = HeadlessRenderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdef915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering frames:   0%|          | 0/204 [00:00<?, ?it/s]UNSUPPORTED (log once): POSSIBLE ISSUE: unit 0 GLD_TEXTURE_INDEX_2D is unloadable and bound to sampler type (Depth) - using zero texture because texture unloadable\n",
      "Rendering frames: 100%|██████████| 204/204 [00:05<00:00, 36.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to /Users/jan.rojc/Documents/MagCode/Data/ccraft_data/temp/output.mp4\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output_m.pkl'\n",
    "out_video = Path(DEFAULTS.data_root) / 'temp' / 'output.mp4'\n",
    "write_video(out_path, out_video, renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03138f70",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
